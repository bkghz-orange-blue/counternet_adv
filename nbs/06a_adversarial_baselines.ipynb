{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp adversarial_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from counterfactual.import_essentials import *\n",
    "from counterfactual.utils import *\n",
    "from counterfactual.training_module import *\n",
    "from counterfactual.net import *\n",
    "from counterfactual.interface import ABCBaseModule, LocalExplainerBase, GlobalExplainerBase\n",
    "from counterfactual.train import train_model\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torchmetrics.functional.classification import accuracy\n",
    "\n",
    "import sklearn\n",
    "from lime.lime_base import LimeBase\n",
    "# import gurobipy as grb\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_config = load_json('assets/configs/adult.json')\n",
    "m_config['lr'] = 0.01\n",
    "t_config = {\n",
    "    \"max_epochs\": 10,\n",
    "    \"gpus\": 0,\n",
    "    \"deterministic\": True,\n",
    "    \"benchmark\": True,\n",
    "    # \"automatic_optimization\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n",
      "hyper parameters: \"batch_size\":     128\n",
      "\"continous_cols\": ['age', 'hours_per_week']\n",
      "\"data_dir\":       assets/data/s_adult.csv\n",
      "\"data_name\":      adult\n",
      "\"dec_dims\":       [10, 10]\n",
      "\"discret_cols\":   ['workclass', 'education', 'marital_status', 'occupation', 'race', 'gender']\n",
      "\"enc_dims\":       [29, 50, 10]\n",
      "\"exp_dims\":       [10, 50]\n",
      "\"lambda_1\":       1.0\n",
      "\"lambda_2\":       0.01\n",
      "\"lambda_3\":       0.2\n",
      "\"loss_func_1\":    mse\n",
      "\"loss_func_2\":    mse\n",
      "\"loss_func_3\":    mse\n",
      "\"lr\":             0.01\n",
      "\"threshold\":      1.0\n",
      "x_cont: (32561, 2), x_cat: (32561, 27)\n",
      "(32561, 29)\n",
      "\n",
      "  | Name  | Type       | Params | In sizes | Out sizes\n",
      "------------------------------------------------------------\n",
      "0 | model | Sequential | 2.1 K  | [1, 29]  | [1, 1]   \n",
      "------------------------------------------------------------\n",
      "2.1 K     Trainable params\n",
      "0         Non-trainable params\n",
      "2.1 K     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b0fb4819ed4208a2e17047ff07f79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586f1a98c5744daaa6febe5544087242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2164ee5281714a2b88939ec8c07e3478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da071fbb98114cf1b05a8ba4deb754ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d259a7f64ef4a60aeb78b3a854a75c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "817ac814a4344fc7ba8cb4ac300ad10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e95b5cc49f548adba3634f16c6d4c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d57b69c3204f56a654d2c3aab5550c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2da5180e264fada71dc76f2f461eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d59d365a5254068ae36d30009f30aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e419185a0c364e6ca36019d4efc0c51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b54c9b9c1e4d7bbe3526d5d424ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = train_model(\n",
    "    BaselineModel(m_config), t_config, logger=None\n",
    ")\n",
    "# model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = model.train_dataset[:]\n",
    "X = X.cpu().detach().numpy()\n",
    "\n",
    "test_X, test_y = model.train_dataset[:]\n",
    "test_X = test_X.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.predict_proba(test_X[:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_fn(x):\n",
    "    if len(x.shape) == 1:\n",
    "        x = torch.from_numpy(x).float().unsqueeze(dim=0)\n",
    "    else:\n",
    "        x = torch.from_numpy(x).float()\n",
    "    prob = model(x).view(-1, 1)\n",
    "    return torch.cat((1-prob, prob), dim=1).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6721392 , 0.32786077]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_fn(test_X[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LimeExplanation(object):\n",
    "    def __init__(self, intercept={}, local_exp={}, score={}, local_pred={}):\n",
    "        self.intercept = intercept\n",
    "        self.local_exp = local_exp\n",
    "        self.score = score\n",
    "        self.local_pred = local_pred\n",
    "\n",
    "    def __str__(self):\n",
    "        return str({\n",
    "            'intercept': self.intercept,\n",
    "            'local_exp': self.local_exp,\n",
    "            'score': self.score,\n",
    "            'local_pred': self.local_pred\n",
    "        })\n",
    "\n",
    "class LimeTabularExplainer(object):\n",
    "    def __init__(self, training_data):\n",
    "        freq = np.sum(training_data, axis=0)\n",
    "        freq = freq / len(training_data)\n",
    "        self.freq = freq\n",
    "        # kernel_width = None\n",
    "        kernel_width = np.sqrt(training_data.shape[1]) * .75\n",
    "        kernel_width = float(kernel_width)\n",
    "\n",
    "        def kernel(d, kernel_width):\n",
    "            return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2))\n",
    "\n",
    "        kernel_fn = partial(kernel, kernel_width=kernel_width)\n",
    "        self.base = LimeBase(kernel_fn)\n",
    "\n",
    "    def generate_neighbors(self, x, cat_arrays, cat_idx, num_samples):\n",
    "        neighbors = np.zeros((num_samples, x.shape[-1]))\n",
    "        cont_perturbed = x[:, :cat_idx] + np.random.normal(0, 0.1, size=(num_samples, cat_idx))\n",
    "        cont_perturbed = np.clip(cont_perturbed, 0., 1.)\n",
    "        _cat_idx = cat_idx\n",
    "        neighbors[:, :cat_idx] = cont_perturbed\n",
    "        for col in cat_arrays:\n",
    "            cat_end_idx = cat_idx + len(col)\n",
    "            # one_hot_idx = np.random.randint(0, len(col), size=(num_samples,))\n",
    "            one_hot_idx = np.random.choice(range(len(col)), size=(num_samples,), p=self.freq[cat_idx: cat_end_idx])\n",
    "            neighbors[:, cat_idx: cat_end_idx] = np.eye(len(col))[one_hot_idx]\n",
    "            cat_idx = cat_end_idx\n",
    "        x = x.reshape(1, -1)\n",
    "        return np.concatenate((x, neighbors), axis=0)\n",
    "\n",
    "    def explain_instance(self,\n",
    "                         x,\n",
    "                         predict_fn,\n",
    "                         cat_arrays,\n",
    "                         cat_idx,\n",
    "                         labels=(1,),\n",
    "                         top_labels=None,\n",
    "                         num_features=200,\n",
    "                         num_samples=5000):\n",
    "        neighbors = self.generate_neighbors(\n",
    "            x, cat_arrays=cat_arrays, cat_idx=cat_idx, num_samples=num_samples)\n",
    "        yss = predict_fn(neighbors) + 1e-6\n",
    "        # map to regression model\n",
    "        yss = - np.log(1 / yss - 1)\n",
    "        distances = sklearn.metrics.pairwise_distances(\n",
    "                neighbors, neighbors[0].reshape(1, -1), metric=\"euclidean\"\n",
    "        ).ravel()\n",
    "\n",
    "        self.class_names = [str(x) for x in range(yss[0].shape[0])]\n",
    "\n",
    "        if top_labels:\n",
    "            labels = np.argsort(yss[0])[-top_labels:]\n",
    "\n",
    "        intercept, local_exp, score, local_pred = {}, {}, {}, {}\n",
    "        for label in labels:\n",
    "            (intercept[label],\n",
    "             local_exp[label],\n",
    "             score[label],\n",
    "             local_pred[label]) = self.base.explain_instance_with_data(\n",
    "                 neighbors, yss, distances, label, num_features,\n",
    "                 model_regressor=sklearn.linear_model.Ridge(alpha=1, fit_intercept=False,)\n",
    "             )\n",
    "        return LimeExplanation(\n",
    "            intercept=intercept,\n",
    "            local_exp=local_exp,\n",
    "            score=score,\n",
    "            local_pred=local_pred\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LocalApprox(object):\n",
    "    def __init__(self, train_X, predict_fn, cat_arrays, cat_idx):\n",
    "        # self.explainer = LimeTabularExplainer(train_X, class_names=['0', '1'], discretize_continuous=False)\n",
    "        self.explainer = LimeTabularExplainer(train_X)\n",
    "        self.predict_fn = predict_fn\n",
    "        self.cat_arrays = cat_arrays\n",
    "        self.cat_idx = cat_idx\n",
    "\n",
    "    def extract_weights(self, x_0, shift=0.1):\n",
    "        # exp = self.explainer.explain_instance(x_0, self.predict_fn, top_labels=1, num_features=200, num_samples=1000)\n",
    "        exp = self.explainer.explain_instance(x_0,\n",
    "                                              self.predict_fn,\n",
    "                                              self.cat_arrays,\n",
    "                                              self.cat_idx,\n",
    "                                            #   top_labels=1,\n",
    "                                              num_features=200,\n",
    "                                              num_samples=5000)\n",
    "        coefs = exp.local_exp[1]\n",
    "\n",
    "        intercept = exp.intercept[1]\n",
    "        coefs = sorted(coefs, key=lambda x: x[0])\n",
    "\n",
    "        w = np.array([e[1] for e in coefs])\n",
    "        # b = intercept - max(self.predict_fn(x_0.reshape(1, -1)).squeeze())\n",
    "        # b = -shift - np.dot(w, x_0)\n",
    "        # print('w: ', w)\n",
    "        # print('b: ', b )\n",
    "\n",
    "        return w, intercept #np.array(b).reshape(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.523104329285651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.70373785, 0.29626215]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_approx = LocalApprox(X, pred_fn, model.cat_arrays, model.cat_idx)\n",
    "coef, intercept = local_approx.extract_weights(test_X[0].reshape(1, -1))\n",
    "all_coef = np.zeros((10, X.shape[1] + 1))\n",
    "# for j in range(10):\n",
    "#     coef, intercept = local_approx.extract_weights(test_X[0], shift=0.1)\n",
    "#     all_coef[j] = np.concatenate((coef, intercept))\n",
    "theta = np.zeros((1, X.shape[1] + 1))\n",
    "sigma = np.zeros((1, X.shape[1] + 1, X.shape[1] + 1))\n",
    "theta[0], sigma[0] = np.mean(all_coef, axis=0), np.cov(all_coef.T)\n",
    "print(np.dot(coef, test_X[0])) #+ intercept\n",
    "pred_fn(test_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ROAR(LocalExplainerBase):\n",
    "    def __init__(self, x: torch.Tensor, model: BaselineModel, n_iters: int = 50, max_delta: float = 0.1):\n",
    "        def pred_fn(x):\n",
    "            if len(x.shape) == 1:\n",
    "                x = torch.from_numpy(x).float().unsqueeze(dim=0)\n",
    "            else:\n",
    "                x = torch.from_numpy(x).float()\n",
    "            prob = model(x).view(-1, 1)\n",
    "            return torch.cat((1-prob, prob), dim=1).cpu().detach().numpy()\n",
    "\n",
    "        super().__init__(x, model)\n",
    "        train_X, y = model.train_dataset[:]\n",
    "        train_X = train_X.cpu().detach().numpy()\n",
    "        self.x = x.clone()\n",
    "        self.lime = LocalApprox(train_X, pred_fn, model.cat_arrays, model.cat_idx)\n",
    "        self.cf = nn.Parameter(x.clone(), requires_grad=True)\n",
    "        self.max_delta = max_delta\n",
    "        self.n_iters = n_iters\n",
    "        self.dim = x.size(-1)\n",
    "\n",
    "    def forward(self):\n",
    "        cf = self.cf * 1.0\n",
    "        return cat_normalize(cf,\n",
    "                             self.model.cat_arrays,\n",
    "                             len(self.model.continous_cols),\n",
    "                             hard=False)\n",
    "\n",
    "    # def adv_attack(self, coef, cf, target):\n",
    "    #     # Model initialization\n",
    "    #     model = grb.Model(\"qcp\")\n",
    "    #     model.params.NonConvex = 2\n",
    "    #     model.setParam('OutputFlag', False)\n",
    "    #     # model.params.threads = 64\n",
    "    #     model.params.IterationLimit = 1e3\n",
    "\n",
    "    #     delta = model.addMVar(self.dim,\n",
    "    #                           lb=float('-inf'), ub=float('inf'),\n",
    "    #                           vtype=grb.GRB.CONTINUOUS, name=\"delta\")\n",
    "    #     # Set objective\n",
    "    #     obj = np.abs(cf @ delta + np.dot(cf, coef) - target)\n",
    "    #     model.setObjective(obj, grb.GRB.MAXIMIZE)\n",
    "    #     # model.setObjective(y, grb.GRB.MAXIMIZE)\n",
    "\n",
    "    #     # set constrains\n",
    "    #     model.addConstr(-self.max_delta <= delta)\n",
    "    #     model.addConstr(delta <= self.max_delta)\n",
    "\n",
    "    #     # optimize\n",
    "    #     model.optimize()\n",
    "    #     return [_delta.x for _delta in delta]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.RMSprop([self.cf], lr=0.1)\n",
    "\n",
    "    def adv_attack(self, coef, cf, target, eps=0.1):\n",
    "        # coef = coef.reshape((-1, 1))\n",
    "        delta = torch.zeros_like(coef).uniform_(-eps, eps)\n",
    "        delta.requires_grad = True\n",
    "        alpha = eps * 1.25\n",
    "\n",
    "        for _ in range(7):\n",
    "            pred = cf @ coef + cf @ delta\n",
    "            y_pred = 1 / (1 + torch.exp(-(pred)))\n",
    "            loss = F.mse_loss(y_pred, target.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            scaled_g = delta.grad.detach()#.sign()\n",
    "            delta.data = l_inf_proj(delta + alpha * scaled_g, eps=eps)\n",
    "            delta.grad.zero_()\n",
    "            if torch.linalg.norm(scaled_g).item() < 1e-3:\n",
    "                break\n",
    "        return delta.detach()\n",
    "\n",
    "    def generate_cf(self):\n",
    "        coef, intercept = self.lime.extract_weights(self.x)\n",
    "        optim = self.configure_optimizers()\n",
    "\n",
    "        # x_0 = torch.from_numpy(x_0)\n",
    "        coef = torch.from_numpy(deepcopy(coef)).float()\n",
    "        coef = coef.reshape((-1, 1))\n",
    "        coef_ = deepcopy(coef)\n",
    "        g = 0\n",
    "        y_pred = 1 / (1 + torch.exp(-(self.x @ coef)))\n",
    "        # print(f'y_lime: {y_pred}, y_model: {self.model(self.x)}')\n",
    "        y_target = torch.ones(y_pred.shape) - torch.round(y_pred)\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "            cf = self()\n",
    "            delta = self.adv_attack(coef, cf.detach(), y_target, self.max_delta)\n",
    "            coef = coef + delta\n",
    "            cf.retain_grad()\n",
    "            # y_pred = cf @ coef\n",
    "            y_pred = 1 / (1 + torch.exp(-(cf @ coef)))\n",
    "            # loss = F.mse_loss(cf @ coef, y_target) + 0.5 * F.mse_loss(self.x, cf)\n",
    "            loss = F.binary_cross_entropy(y_pred, y_target) + 0.5 * F.mse_loss(self.x, cf)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            # check lime\n",
    "            # if i % 10 == 0:\n",
    "            #     print(f'y_lime: {1 / (1 + torch.exp(-(cf @ coef)))}, y_model: {self.model(cf)}')\n",
    "\n",
    "        cf = self.cf * 1.0\n",
    "        return cat_normalize(cf.detach(),\n",
    "                             self.model.cat_arrays,\n",
    "                             len(self.model.continous_cols),\n",
    "                             hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34246576, 0.4489796 , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_lime: tensor([[0.2667]], grad_fn=<MulBackward0>), y_model: tensor([0.3372], grad_fn=<SqueezeBackward1>)\n",
      "y_lime: tensor([[0.9538]], grad_fn=<MulBackward0>), y_model: tensor([0.7967], grad_fn=<SqueezeBackward1>)\n",
      "y_lime: tensor([[0.9568]], grad_fn=<MulBackward0>), y_model: tensor([0.7798], grad_fn=<SqueezeBackward1>)\n",
      "y_lime: tensor([[0.9266]], grad_fn=<MulBackward0>), y_model: tensor([0.7985], grad_fn=<SqueezeBackward1>)\n",
      "y_lime: tensor([[0.9416]], grad_fn=<MulBackward0>), y_model: tensor([0.7875], grad_fn=<SqueezeBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(9.8499)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from counterfactual.evaluate import proximity\n",
    "\n",
    "i = 1\n",
    "test_X = torch.tensor(test_X)\n",
    "roar = ROAR(test_X[i].float().reshape(1, -1), model)\n",
    "cf = roar.generate_cf(50)\n",
    "proximity(test_X[i], cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_X[i]) == model.predict(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROAR(object):\n",
    "    \"\"\" Class for generate counterfactual samples for framework: AR \"\"\"\n",
    "\n",
    "    def __init__(self, data, coef, intercept, lmbda=0.1, sigma_min=None, sigma_max=0.5, alpha=0.1, dist_type='l2', max_iter=20, padding=False):\n",
    "        \"\"\" Parameters\n",
    "\n",
    "        Args:\n",
    "            data: data to generate counterfactuals\n",
    "            model_trained: model trained on original data\n",
    "            padding: True if we padding 1 at the end of instances\n",
    "        \"\"\"\n",
    "        self.data = np.concatenate((data, np.ones(len(data)).reshape(-1, 1)), axis=1)\n",
    "        self.coef = np.concatenate((coef, intercept))\n",
    "        self.lmbda = lmbda\n",
    "        self.alpha = alpha\n",
    "        self.dim = self.data.shape[1]\n",
    "        self.dist_type = dist_type\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    # def objective_func(self, coef, x, x_0):\n",
    "    #     \"\"\" Loss function - mse or log loss\n",
    "\n",
    "    #     Args:\n",
    "    #         coef: model params\n",
    "    #         x: a single input\n",
    "    #         x_0; original input\n",
    "    #         loss_type: mse or log loss\n",
    "    #         dist_type: l1 or l2\n",
    "\n",
    "    #     Returns:\n",
    "    #         output: output of objective function\n",
    "    #     \"\"\"\n",
    "    #     dist = torch.linalg.norm(x - x_0)\n",
    "    #     loss = (torch.dot(coef, x) - 1) ** 2\n",
    "    #     output = loss + self.lmbda * dist\n",
    "    #     return output\n",
    "\n",
    "    def find_optimal_sigma(self, coef, x):\n",
    "        \"\"\" Find value of sigma at each step\n",
    "\n",
    "        Args:\n",
    "            coef: coef of model\n",
    "            x: input\n",
    "\n",
    "        Returns:\n",
    "            x_opt: x at step t + 1\n",
    "        \"\"\"\n",
    "        # Model initialization\n",
    "        model = grb.Model(\"qcp\")\n",
    "        model.params.NonConvex = 2\n",
    "        model.setParam('OutputFlag', False)\n",
    "        model.params.threads = 64\n",
    "        model.params.IterationLimit = 1e3\n",
    "\n",
    "        sigma = model.addMVar(self.dim, lb=float('-inf'), ub=float('inf'), vtype=grb.GRB.CONTINUOUS, name=\"sigma\")\n",
    "        sigma_norm = model.addMVar(1, lb=float('-inf'), ub=float('inf'), vtype=grb.GRB.CONTINUOUS, name=\"sigma_norm\")\n",
    "\n",
    "        # Set objective\n",
    "        obj = x @ sigma + np.dot(x, coef)\n",
    "        model.setObjective(obj, grb.GRB.MAXIMIZE)\n",
    "\n",
    "        # Constraints\n",
    "        if self.sigma_min:\n",
    "            model.addConstr(self.sigma_min <= sigma)\n",
    "            model.addConstr(sigma <= self.sigma_max)\n",
    "        else:\n",
    "            model.addConstr(sigma_norm @ sigma_norm == sigma @ sigma)\n",
    "            model.addConstr(sigma_norm <= self.sigma_max)\n",
    "            model.addConstr(sigma_norm >= 0)\n",
    "\n",
    "        model.optimize()\n",
    "\n",
    "        sigma_hat = np.zeros(self.dim)\n",
    "\n",
    "        for i in range(self.dim):\n",
    "            sigma_hat[i] = sigma[i].x\n",
    "        \n",
    "        return sigma_hat\n",
    "\n",
    "\n",
    "    def fit_instance(self, x_0):\n",
    "        x_t = torch.from_numpy(x_0.copy())\n",
    "        x_t.requires_grad = True\n",
    "        x_0 = torch.from_numpy(x_0)\n",
    "        coef = torch.from_numpy(self.coef.copy())\n",
    "        coef_ = torch.from_numpy(self.coef.copy())\n",
    "        ord = None if self.dist_type=='l2' else 1\n",
    "        g = 0\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            sigma_hat = self.find_optimal_sigma(coef, x_t.detach().numpy())\n",
    "            coef_ = coef + torch.from_numpy(sigma_hat)\n",
    "            x_t.retain_grad()\n",
    "            out = (1 / (1 + torch.exp(-torch.dot(coef_, x_t))) - 1) ** 2 + self.lmbda * torch.linalg.norm(x_t - x_0, ord=ord)\n",
    "            out.backward()\n",
    "            g = x_t.grad\n",
    "            x_t = x_t - self.alpha * g\n",
    "            \n",
    "            print(torch.dot(coef, x_t))\n",
    "            if torch.linalg.norm(self.alpha * g).item() < 1e-3:\n",
    "                break\n",
    "        return x_t.detach().numpy()\n",
    "\n",
    "\n",
    "    def fit_data(self, data):\n",
    "        \"\"\" Fit linear recourse action with all instances\n",
    "\n",
    "        Args:\n",
    "            data: all the input instances\n",
    "\n",
    "        Returns:\n",
    "            counterfactual_samples: counterfactual of instances in dataset\n",
    "        \"\"\"\n",
    "        l = len(data)\n",
    "        counterfactual_samples = np.zeros((l, self.dim))\n",
    "\n",
    "        for i in tqdm(range(l)):\n",
    "            counterfactual_samples[i] = self.fit_instance(data[i])\n",
    "\n",
    "        return counterfactual_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "roar = ROAR(test_X[:5], coef.squeeze(), intercept, \n",
    "    1e-6, sigma_max=0.1, alpha=0.5, dist_type='l1', max_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0988, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0976, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0963, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0951, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0939, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0927, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0915, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0902, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0890, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0878, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0866, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0854, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0842, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0830, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0818, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0806, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0794, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0782, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0770, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0758, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0746, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0734, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0722, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0710, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0699, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0687, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0675, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0663, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0651, dtype=torch.float64, grad_fn=<DotBackward>)\n",
      "Changed value of parameter NonConvex to 2\n",
      "   Prev: -1  Min: -1  Max: 2  Default: -1\n",
      "tensor(-0.0639, dtype=torch.float64, grad_fn=<DotBackward>)\n"
     ]
    }
   ],
   "source": [
    "counterfactual_roar = roar.fit_instance(roar.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.41906118e-01,  4.49447453e-01, -4.99838982e-03, -5.34240059e-03,\n",
       "        9.96979285e-01, -7.93637303e-03, -2.67477231e-03, -3.61381126e-04,\n",
       "       -4.66741147e-03,  9.99381525e-01,  4.65992500e-03, -3.80545420e-03,\n",
       "        2.64909015e-03,  1.24679086e-02, -3.06179412e-03,  1.00470332e+00,\n",
       "       -8.09919314e-03, -1.36587863e-02, -2.54365250e-04,  1.25376117e+00,\n",
       "       -9.74689441e-03, -1.93682848e-04, -5.33238516e-03, -2.95295587e-03,\n",
       "        2.79555974e-03, -3.77848308e-03,  1.00000000e+00,  1.71915664e-02,\n",
       "        1.25392747e+00,  6.40389362e-01])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counterfactual_roar"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1aebd4a71fcec916e49c5e2d294321100f54b5d9bbeca9249da939e91b36ccc5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
