{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp training_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from logging import raiseExceptions\n",
    "from typing import Tuple\n",
    "from fastcore.foundation import L\n",
    "from counterfactual.import_essentials import *\n",
    "from counterfactual.utils import *\n",
    "from torchmetrics.functional import accuracy\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, OneHotEncoder\n",
    "import higher\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from counterfactual.interface import ABCBaseModule, GlobalExplainerBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pl version: 1.1.0\n",
      "torch version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"pl version: {pl.__version__}\")\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.02 s, sys: 284 ms, total: 1.31 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dummy_data = pd.read_csv('../data/dummy_data.csv')\n",
    "adult_data = load_adult_income_dataset('../data/adult.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[39., 40.],\n",
       "       [50., 13.],\n",
       "       [38., 40.],\n",
       "       ...,\n",
       "       [58., 40.],\n",
       "       [22., 20.],\n",
       "       [52., 40.]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "scalar = StandardScaler()\n",
    "cont = scalar.fit_transform(adult_data[['age', 'hours_per_week']])\n",
    "scalar.inverse_transform(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Government', 'Bachelors', 'Single', 'White-Collar', 'White',\n",
       "        'Male'],\n",
       "       ['Self-Employed', 'Bachelors', 'Married', 'White-Collar', 'White',\n",
       "        'Male'],\n",
       "       ['Private', 'HS-grad', 'Divorced', 'Blue-Collar', 'White', 'Male'],\n",
       "       ...,\n",
       "       ['Private', 'HS-grad', 'Widowed', 'White-Collar', 'White',\n",
       "        'Female'],\n",
       "       ['Private', 'HS-grad', 'Single', 'White-Collar', 'White', 'Male'],\n",
       "       ['Self-Employed', 'HS-grad', 'Married', 'White-Collar', 'White',\n",
       "        'Female']], dtype=object)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "cat  = enc.fit_transform(adult_data[['workclass','education', 'marital_status', \n",
    "            'occupation','race', 'gender']])\n",
    "enc.inverse_transform(cat.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModule(pl.LightningModule, ABCBaseModule):\n",
    "    _DEFAULT_PARAMS = {\n",
    "        'encoder': None,\n",
    "        'normalizer': None,\n",
    "        'enc_dims': [],\n",
    "        'dec_dims': [],\n",
    "        'exp_dims': [],\n",
    "        'lr': 3e-3,\n",
    "        'batch_size': 128,\n",
    "        'sample_frac': None,\n",
    "        'lambda_1': 1.0,\n",
    "        'lambda_2': 1.0,\n",
    "        'lambda_3': 1.0,\n",
    "        'threshold': 0.5,\n",
    "        'smooth_y': True\n",
    "    }\n",
    "\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(config)\n",
    "\n",
    "        # read data\n",
    "        self.data = pd.read_csv(Path(config['data_dir']))\n",
    "\n",
    "        # update default parameters\n",
    "        self._update_params(config)\n",
    "\n",
    "        # update cat_idx\n",
    "        self.cat_idx = len(self.continous_cols)\n",
    "\n",
    "        # log graph\n",
    "        self.example_input_array = torch.randn((1, self.enc_dims[0]))\n",
    "\n",
    "\n",
    "    def _update_params(self, config: Dict[str, Any]):\n",
    "        self._DEFAULT_PARAMS.update(config)\n",
    "        for k, v in self._DEFAULT_PARAMS.items():\n",
    "            if k in ['loss_func_1', 'loss_func_2', 'loss_func_3', 'adv_loss_func']:\n",
    "                setattr(self, k, get_loss_functions(config[k]) )\n",
    "            else:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def check_cols(self):\n",
    "        self.data = self.data.astype({col: np.float for col in self.continous_cols})\n",
    "\n",
    "    def training_epoch_end(self, outs):\n",
    "        if self.current_epoch == 0:\n",
    "            self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def transform(self, x, return_tensor=True):\n",
    "        assert isinstance(x, pd.DataFrame)\n",
    "        x_cont = self.normalizer.transform(x[self.continous_cols]) if self.continous_cols else np.array([[] for _ in range(len(x))])\n",
    "        x_cat = self.encoder.transform(x[self.discret_cols]) if self.discret_cols else np.array([[] for _ in range(len(x))])\n",
    "        x = np.concatenate((x_cont, x_cat), axis=1)\n",
    "        return torch.from_numpy(x).float() if return_tensor else x\n",
    "\n",
    "    def inverse_transform(self, x, return_tensor=True):\n",
    "        \"\"\"x should be a transformed tensor\"\"\"\n",
    "        cat_idx = self.cat_idx\n",
    "        # inverse transform\n",
    "        x_cont_inv = self.normalizer.inverse_transform(x[:, :cat_idx].cpu())\n",
    "        x_cat_inv = self.encoder.inverse_transform(x[:, cat_idx:].cpu()) if self.discret_cols else np.array([[] for _ in range(len(x))])\n",
    "        x = np.concatenate((x_cont_inv, x_cat_inv), axis=1)\n",
    "        return torch.from_numpy(x).float() if return_tensor else x\n",
    "\n",
    "    def check_cont_robustness(self, x, c, c_y):\n",
    "        cat_idx = self.cat_idx\n",
    "        # inverse transform\n",
    "        x_cont_inv = self.normalizer.inverse_transform(x[:, :cat_idx].cpu())\n",
    "        c_cont_inv = self.normalizer.inverse_transform(c[:, :cat_idx].cpu())\n",
    "        # calculate the diff between x and c\n",
    "        cont_diff = np.abs(x_cont_inv - c_cont_inv) < self.threshold\n",
    "        # total nums of differences\n",
    "        total_diffs = np.sum(cont_diff.any(axis=1))\n",
    "        # new continous cf\n",
    "        c_cont_hat = np.where(cont_diff, x_cont_inv, c_cont_inv)\n",
    "        c[:, :cat_idx] = torch.from_numpy(self.normalizer.transform(c_cont_hat))\n",
    "        c_y_hat = self.predict(c)\n",
    "        return ((c_y_hat > .5) != (c_y > .5)).sum(), total_diffs\n",
    "\n",
    "    def cat_normalize(self, c, hard=False):\n",
    "        # categorical feature starting index\n",
    "        return cat_normalize(c, self.cat_arrays, self.cat_idx, hard=hard)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        def split_x_and_y(data: pd.DataFrame):\n",
    "            X = data[data.columns[:-1]]\n",
    "            y = data[data.columns[-1]]\n",
    "            return X, y\n",
    "\n",
    "        X, y = split_x_and_y(self.data)\n",
    "\n",
    "        # preprocessing\n",
    "        if self.normalizer:\n",
    "            X_cont = self.normalizer.transform(X[self.continous_cols])\n",
    "        else:\n",
    "            self.normalizer = MinMaxScaler()\n",
    "            X_cont = self.normalizer.fit_transform(\n",
    "                X[self.continous_cols]) if self.continous_cols else np.array([[] for _ in range(len(X))])\n",
    "\n",
    "        if self.encoder:\n",
    "            X_cat = self.encoder.transform(X[self.discret_cols])\n",
    "        else:\n",
    "            self.encoder = OneHotEncoder(sparse=False)\n",
    "            X_cat = self.encoder.fit_transform(\n",
    "                X[self.discret_cols]) if self.discret_cols else np.array([[] for _ in range(len(X))])\n",
    "        X = np.concatenate((X_cont, X_cat), axis=1)\n",
    "        pl_logger.info(f\"x_cont: {X_cont.shape}, x_cat: {X_cat.shape}\")\n",
    "\n",
    "        self.cat_arrays = self.encoder.categories_ if self.discret_cols else []\n",
    "        pl_logger.info(X.shape)\n",
    "        assert X.shape[-1] == self.enc_dims[0], f'The input dimension X (shape: {X.shape[-1]})  != encoder_dims[0]: {self.enc_dims}'\n",
    "\n",
    "        # prepare train & test\n",
    "        train_X, test_X, train_y, test_y = train_test_split(X, y.to_numpy(), shuffle=False)\n",
    "        if self.sample_frac:\n",
    "            train_size = int(len(train_X) * self.sample_frac)\n",
    "            train_X, train_y = train_X[:train_size], train_y[:train_size]\n",
    "        self.train_dataset = NumpyDataset(train_X, train_y)\n",
    "        self.val_dataset = NumpyDataset(test_X, test_y)\n",
    "        self.test_dataset = self.val_dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=True, num_workers=0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size * 4,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          pin_memory=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 1, 0])\n",
    "x[x == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0800, 0.8633, 0.8413, 0.1227])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(x==1, torch.rand(4) * 0.15 + 0.8, torch.rand(4) * 0.15 + 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils\n",
    "def uniform(shape: tuple, r1: float, r2: float, device=None, requires_grad=False):\n",
    "    assert r1 < r2\n",
    "    return (r2 - r1) * torch.rand(*shape, device=device, requires_grad=requires_grad) + r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PredictiveTrainingModule(BaseModule):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def prepare_lime_configs(self):\n",
    "        features_name = self.continous_cols + self.discret_cols\n",
    "\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, *x):\n",
    "        return self.model_forward(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"x has not been preprocessed\"\"\"\n",
    "        y_hat = self(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        y = smooth_y(y)\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        # Logging to TensorBoard by default\n",
    "        self.log('train/train_loss_1', loss, on_step=True, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        *x, y = batch\n",
    "        # fwd\n",
    "        y_hat = self(*x)\n",
    "        # loss\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        score = accuracy(y_hat > .5, y.int())\n",
    "        return {'score': score, 'val_loss': loss}\n",
    "\n",
    "    def validation_epoch_end(self, val_outs):\n",
    "        avg_loss = torch.stack([output['val_loss'] for output in val_outs]).mean()\n",
    "        avg_score = torch.stack([output['score'] for output in val_outs]).mean()\n",
    "        self.log('val/val_loss', avg_loss)\n",
    "        self.log('val/pred_accuracy', avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils\n",
    "def hinge_loss(input, target):\n",
    "    \"\"\"\n",
    "    reference:\n",
    "    - https://github.com/interpretml/DiCE/blob/a772c8d4fcd88d1cab7f2e02b0bcc045dc0e2eab/dice_ml/explainer_interfaces/dice_pytorch.py#L196-L202\n",
    "    - https://en.wikipedia.org/wiki/Hinge_loss\n",
    "    \"\"\"\n",
    "    input = torch.log((abs(input - 1e-6) / (1 - abs(input - 1e-6))))\n",
    "    all_ones = torch.ones_like(target)\n",
    "    target = 2 * target - all_ones\n",
    "    loss = all_ones - torch.mul(target, input)\n",
    "    loss = F.relu(loss)\n",
    "    return torch.norm(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.2904],\n",
      "        [0.3001],\n",
      "        [0.0508],\n",
      "        [0.3926]]) \n",
      "hinge: 8.592495918273926 \n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([[0.6, 0.7, 0.1, 0.8]])\n",
    "x = torch.rand(4, 1)\n",
    "target = torch.tensor([[1, 1, 0, 1]])\n",
    "\n",
    "print(f\"x: {x} \")\n",
    "print(f\"hinge: {hinge_loss(x, target)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9465, -0.0229,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "          0.0000,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(1, 20)\n",
    "logits[:, 2:] = F.gumbel_softmax(logits[:, 2:], hard=True)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export utils\n",
    "def cat_normalize(c, cat_arrays, cat_idx, hard=False):\n",
    "    # categorical feature starting index\n",
    "    for col in cat_arrays:\n",
    "        cat_end_idx = cat_idx + len(col)\n",
    "        if hard:\n",
    "            c[:, cat_idx: cat_end_idx] = F.gumbel_softmax(c[:, cat_idx: cat_end_idx].clone(), hard=hard)\n",
    "        else:\n",
    "            c[:, cat_idx: cat_end_idx] = F.softmax(c[:, cat_idx: cat_end_idx].clone(), dim=-1)\n",
    "        cat_idx = cat_end_idx\n",
    "    return c\n",
    "\n",
    "def l1_mean(x, c):\n",
    "    return F.l1_loss(x, c, reduction='mean') / x.abs().mean() # MAD\n",
    "\n",
    "_loss_functions = {\n",
    "    'cross_entropy': F.binary_cross_entropy,\n",
    "    'l1': F.l1_loss,\n",
    "    'l1_mean': l1_mean,\n",
    "    'mse': F.mse_loss\n",
    "}\n",
    "\n",
    "def get_loss_functions(f_name: str):\n",
    "    assert f_name in _loss_functions.keys(), f'function name \"{f_name}\" is not in the loss function list {_loss_functions.keys()}'\n",
    "    return _loss_functions[f_name]\n",
    "\n",
    "_optimizers = {\n",
    "    'adam': torch.optim.Adam\n",
    "}\n",
    "\n",
    "def get_optimizers(o_name: str):\n",
    "    assert o_name in _optimizers.keys(), f'optimizer name \"{o_name}\" is not in the optimizer list {_optimizers.keys()}'\n",
    "    return _optimizers[o_name]\n",
    "\n",
    "def smooth_y(y):\n",
    "    return torch.where(y == 1,\n",
    "                       uniform(y.size(), 0.8, 0.95, device=y.device),\n",
    "                       uniform(y.size(), 0.05, 0.2, device=y.device))\n",
    "\n",
    "def use_grad(*models, requires_grad: bool):\n",
    "    for model in models:\n",
    "        assert isinstance(model, nn.Module), f\"{model} is not a `nn.Module` \"\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class CounterfactualTrainingModule(BaseModule, GlobalExplainerBase):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hard: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"return predicted probability and counterfactual explanation\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input instance (processed by `self.encoder` and `self.normalizer`)\n",
    "            hard (bool, optional): categorical features in cf is in one-hot-encoding or not. Defaults to False (in softmax format).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: predicted probability and counterfactual explanation\n",
    "\n",
    "        \"\"\"\n",
    "        y, cf = self.model_forward(x)\n",
    "        cf = self.cat_normalize(cf, hard=hard)\n",
    "        return y, cf\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"return predicted label\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): input instance (processed by `self.encoder` and `self.normalizer`)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: predicted label\n",
    "        \"\"\"\n",
    "        y_hat, c = self.model_forward(x)\n",
    "        return torch.round(y_hat)\n",
    "\n",
    "    def generate_cf(self, x, clamp=False):\n",
    "        self.freeze()\n",
    "        y, c = self.model_forward(x)\n",
    "        if clamp:\n",
    "            c = torch.clamp(c, 0., 1.)\n",
    "        return self.cat_normalize(c, hard=True)\n",
    "\n",
    "    def _loss_functions(self, x, c, y, y_hat, y_prime=None, y_prime_mode='predicted', is_val=False):\n",
    "        \"\"\"\n",
    "        x: input value\n",
    "        c: conterfactual example\n",
    "        y: ground truth\n",
    "        y_hat: predicted result\n",
    "        y_prime_mode: 'label' or 'predicted'\n",
    "        \"\"\"\n",
    "        # flip zero/one\n",
    "        if y_prime == None:\n",
    "            if y_prime_mode == 'label':\n",
    "                y_prime = torch.ones(y.shape) - y\n",
    "            elif y_prime_mode == 'predicted':\n",
    "                y_prime = (y_hat < .5).clone().detach().float()\n",
    "\n",
    "        c_y, _ = self(c)\n",
    "        # loss functions\n",
    "        if self.smooth_y and not is_val:\n",
    "            y = smooth_y(y)\n",
    "            y_prime = smooth_y(y_prime)\n",
    "        l_1 = self.loss_func_1(y_hat, y)\n",
    "        l_2 = self.loss_func_2(x, c)\n",
    "        l_3 = self.loss_func_3(c_y, y_prime)\n",
    "\n",
    "        return l_1, l_2, l_3\n",
    "\n",
    "    def _loss_compute(self, l_1, l_2, l_3):\n",
    "        return self.lambda_1 * l_1 + self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "\n",
    "    def _logging_loss(self, l_1, l_2, l_3, stage: str, on_step: bool = False):\n",
    "        self.log(f'{stage}/{stage}_loss_1', l_1, on_step=on_step, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "        self.log(f'{stage}/{stage}_loss_2', l_2, on_step=on_step, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "        self.log(f'{stage}/{stage}_loss_3', l_3, on_step=on_step, on_epoch=True, prog_bar=False, logger=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_1 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        opt_2 = torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=self.lr)\n",
    "        return (opt_1, opt_2)\n",
    "\n",
    "    def predictor_loss(self, x, y):\n",
    "        # fwd\n",
    "        y_hat, c = self(x)\n",
    "        y = smooth_y(y) if self.smooth_y else y\n",
    "        return self.loss_func_1(y_hat, y)\n",
    "\n",
    "    def predictor_step(self, l_1, opt):\n",
    "        p_loss = self.lambda_1 * l_1\n",
    "        self.manual_backward(p_loss, opt)\n",
    "        nn.utils.clip_grad_norm_(self.parameters(), 0.5)\n",
    "        opt.step()\n",
    "        return p_loss\n",
    "\n",
    "    def explainer_loss(self, x):\n",
    "        y_hat, cf = self(x)\n",
    "        cf_y, _ = self.model_forward(cf)\n",
    "        y_prime =  (1. - torch.round(y_hat)).clone().detach()\n",
    "\n",
    "        if self.smooth_y:\n",
    "            y_prime = smooth_y(y_prime)\n",
    "        l_2 = self.loss_func_2(x, cf)\n",
    "        l_3 = self.loss_func_3(cf_y, y_prime)\n",
    "        return l_2, l_3\n",
    "\n",
    "    def explainer_step(self, l_2, l_3, opt):\n",
    "        e_loss = self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "        self.manual_backward(e_loss, opt)\n",
    "        nn.utils.clip_grad_norm_(self.parameters(), 0.5)\n",
    "        opt.step()\n",
    "        return e_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "        # fwd\n",
    "        y_hat, c = self(x)\n",
    "        # define optimizers\n",
    "        opt_1, opt_2 = self.optimizers()\n",
    "\n",
    "        ##########################\n",
    "        # predictor optimization #\n",
    "        ##########################\n",
    "        l_1 = self.predictor_loss(x, y)\n",
    "        p_loss = self.predictor_step(l_1, opt=opt_1)\n",
    "        self.log('train/p_loss', p_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        ##########################\n",
    "        # explainer optimization #\n",
    "        ##########################\n",
    "        l_2, l_3 = self.explainer_loss(x)\n",
    "        e_loss = self.explainer_step(l_2, l_3, opt=opt_2)\n",
    "        self.log('train/e_loss', e_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='train', on_step=False)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # batch\n",
    "        x, y = batch\n",
    "        # fwd\n",
    "        y_hat, c = self(x, hard=True)\n",
    "        c_y, _ = self(c)\n",
    "        # loss\n",
    "        l_1, l_2, l_3 = self._loss_functions(x, c, y, y_hat, is_val=True)\n",
    "        loss = self.lambda_3 * l_3 + self.lambda_2 * l_2\n",
    "        # logging val loss\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='val')\n",
    "\n",
    "        # metrics\n",
    "        cat_idx = len(self.continous_cols)\n",
    "\n",
    "        pred_acc = accuracy(y_hat > .5, y.int())\n",
    "        cf_proximity = torch.abs(x - c).sum(dim=1).mean()\n",
    "        cf_acc = accuracy(c_y > .5, (y_hat < .5).int())\n",
    "\n",
    "        # print counterfactual results\n",
    "        # log = self._logging_cf_results(x, c, y, y_hat, c_y)\n",
    "        log = None\n",
    "\n",
    "        # logging robustness on manipulating small\n",
    "        diffs, total_num = self.check_cont_robustness(x, c, c_y)\n",
    "        sensitivity = 1 - diffs / total_num if total_num != 0 else 1.\n",
    "\n",
    "        metrics = {\n",
    "            'val/val_loss': loss, 'val/pred_accuracy': pred_acc,\n",
    "            'val/cf_proximity': cf_proximity, 'val/sensitivity': sensitivity,\n",
    "            'val/cf_accuracy': cf_acc\n",
    "        }\n",
    "\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "\n",
    "class AdvCounterfactualTrainingModule(CounterfactualTrainingModule, ABC):\n",
    "    def __init__(self, config: Dict):\n",
    "        super().__init__(config)\n",
    "        self._DEFAULT_PARAMS.update({\n",
    "            'epsilon': 0.1,\n",
    "            'alpha': 'auto',\n",
    "        })\n",
    "        self._update_params(config)\n",
    "\n",
    "    def _perturb_input(self, x: torch.Tensor, delta: torch.Tensor):\n",
    "        raise NotImplementedError(\"_perturb_input(x, delta) is not implemented\")\n",
    "\n",
    "class AdvBiLevelCounterfactualTrainingModule(AdvCounterfactualTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self._DEFAULT_PARAMS.update({\n",
    "            'eps_scheduler': 'linear',\n",
    "            'n_steps': 7,\n",
    "            'k': 2,\n",
    "            'adv_loss_func': \"cross_entropy\",\n",
    "            'adv_val_models': None\n",
    "        })\n",
    "        self._update_params(config)\n",
    "        self.glob_epsilon = self.epsilon\n",
    "        self.epsilon = 0.\n",
    "\n",
    "    def predictor_loss(self, x, y, target_model_name='org'):\n",
    "        # fwd\n",
    "        if target_model_name is 'org':\n",
    "            y_hat, _ = self.model.model_forward(x)\n",
    "        elif target_model_name is 'dup':\n",
    "            y_hat, _ = self.model_dup.model_forward(x)\n",
    "        else:\n",
    "            raise ValueError(f\"target should be one of ['org', 'dup'], but got `{target_model_name}`\")\n",
    "        y = smooth_y(y) if self.smooth_y else y\n",
    "        return self.loss_func_1(y_hat, y)\n",
    "\n",
    "    def _perturb_input(self, x: torch.Tensor, delta: torch.Tensor, model_dup: Optional[nn.Module] = None):\n",
    "        if model_dup is None:\n",
    "            model_dup = self.model_dup\n",
    "        if torch.norm(delta) != 0:\n",
    "            x = self.cat_normalize(x + delta)\n",
    "        y_hat, cf = self.model.model_forward(x)\n",
    "        cf = self.cat_normalize(cf, hard=False)\n",
    "        cf_y, _ = model_dup.model_forward(cf)\n",
    "        y_prime = (1. - torch.round(y_hat)).clone().detach()\n",
    "        return y_hat, cf, cf_y, y_prime\n",
    "\n",
    "    def explainer_loss(self, x: torch.Tensor, delta: Optional[torch.Tensor]=None, model_dup: Optional[nn.Module] = None):\n",
    "        if delta is None:\n",
    "            delta = torch.tensor([0.])\n",
    "        y_hat, cf, cf_y, y_prime = self._perturb_input(x, delta, model_dup)\n",
    "        self.log(\"train/robust_validity\", accuracy(cf_y, y_prime.int()), on_step=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        if self.smooth_y:\n",
    "            y_prime = smooth_y(y_prime)\n",
    "        l_2 = self.loss_func_2(x, cf)\n",
    "        l_3 = self.loss_func_3(cf_y, y_prime)\n",
    "        return l_2, l_3\n",
    "\n",
    "    def adv_loss(self, x: torch.Tensor, delta: Optional[torch.Tensor]=None, model_dup: Optional[nn.Module] = None):\n",
    "        if delta is None:\n",
    "            delta = torch.tensor([0.])\n",
    "        y_hat, cf, cf_y, y_prime = self._perturb_input(x, delta, model_dup)\n",
    "        adv_loss = self.adv_loss_func(cf_y, y_prime)\n",
    "        return adv_loss\n",
    "\n",
    "    def fast_adv_training(self, x, epsilon):\n",
    "        alpha = epsilon * 1.25\n",
    "        # init delta\n",
    "        delta = torch.zeros_like(x).uniform_(-epsilon, epsilon)\n",
    "        delta.requires_grad = True\n",
    "        # explainer loss and calculate delta.grad\n",
    "        l_2, l_3 = self.explainer_loss(x, delta)\n",
    "        loss = self.lambda_2 * l_2 + self.lambda_3 * l_3\n",
    "        loss.backward()\n",
    "        # fast-sign gradient descent\n",
    "        scaled_g = delta.grad.detach().sign()\n",
    "        delta.data = l_inf_proj(delta + alpha * scaled_g, eps=epsilon, cat_idx=self.cat_idx)\n",
    "        delta.grad.zero_()\n",
    "        return delta.detach()\n",
    "\n",
    "    def bilevel_adv_training(self, batch, _opt):\n",
    "        x, y = batch\n",
    "        # alpha = self.epsilon * 1.25\n",
    "        alpha = self.epsilon * 2.5 / self.n_steps\n",
    "        # init delta randomly\n",
    "        delta = torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n",
    "        delta.requires_grad = True\n",
    "\n",
    "        use_grad(self, requires_grad=False)\n",
    "        use_grad(self.model_dup.predictor, self.model_dup.pred_linear, requires_grad=True)\n",
    "        opt = torch.optim.Adam(self.model_dup.parameters(), lr=self.lr)\n",
    "        adv_loss = 0.\n",
    "        val_loss = self.adv_loss(x, delta=None, model_dup=self.model)\n",
    "        # _, val_loss = self.explainer_loss(x)\n",
    "\n",
    "        for i in range(self.n_steps):\n",
    "            with higher.innerloop_ctx(self.model_dup, opt, copy_initial_weights=True) as (fmodel, diffopt):\n",
    "                for j in range(self.k):\n",
    "                    _x = self.cat_normalize(x + delta)\n",
    "                    y_hat, _ = fmodel(_x)\n",
    "\n",
    "                    loss = self.loss_func_1(y_hat, y)\n",
    "                    params = diffopt.step(loss)\n",
    "\n",
    "                if i < self.n_steps - 1:\n",
    "                    loss = self.adv_loss(x, delta=None, model_dup=fmodel)\n",
    "                    loss.backward()\n",
    "\n",
    "                    l = len(x.shape) - 1\n",
    "                    g = delta.grad.detach()\n",
    "                    g_norm = torch.norm(g.view(g.shape[0], -1), dim=1).view(-1, *([1]*l))\n",
    "                    scaled_g = g / (g_norm + 1e-10)\n",
    "                    delta.data = l2_proj(delta + alpha * scaled_g, eps=self.epsilon, cat_idx=self.cat_idx)\n",
    "\n",
    "                    # scaled_g = delta.grad.detach().sign()\n",
    "                    # delta.data = l_inf_proj(delta + alpha * scaled_g, eps=self.epsilon, cat_idx=self.cat_idx)\n",
    "                    delta.grad.zero_()\n",
    "\n",
    "                self.model_dup.load_state_dict(fmodel.state_dict())\n",
    "                adv_loss = loss.detach()\n",
    "        self.log('train/adv_loss', adv_loss, on_step=True)\n",
    "        self.log('train/adv_loss_increment', adv_loss - val_loss, on_step=True)\n",
    "\n",
    "\n",
    "    # def bilevel_adv_training(self, batch, opt):\n",
    "    #     x, y = batch\n",
    "    #     delta = torch.zeros_like(x)\n",
    "    #     for i in range(self.n_steps):\n",
    "    #         # STEP 1: adversarial training to invalidate CF generator\n",
    "    #         use_grad(self, requires_grad=False)\n",
    "    #         if self.epsilon == 0.:\n",
    "    #             # only apply unrolling to the predictor\n",
    "    #             delta = 0.\n",
    "    #         else:\n",
    "    #             delta = self.fast_adv_training(x, self.epsilon)\n",
    "\n",
    "    #         # STEP 2: update predictor\n",
    "    #         use_grad(self.model_dup.predictor, self.model_dup.pred_linear, requires_grad=True)\n",
    "    #         for _ in range(self.k):\n",
    "    #             x = self.cat_normalize(x + delta)\n",
    "    #             p_loss = self.predictor_loss(x, y, 'dup')\n",
    "    #             self.manual_backward(p_loss, opt)\n",
    "    #             nn.utils.clip_grad_norm_(self.model_dup.parameters(), 0.5)\n",
    "    #             opt.step()\n",
    "\n",
    "    def schedule_eps(self):\n",
    "        num_batches = len(self.train_dataloader())\n",
    "        max_steps = self.trainer.max_epochs * num_batches\n",
    "\n",
    "        self.epsilon = self.glob_epsilon * ((self.global_step + 1) / max_steps)\n",
    "        self.log('train/eps', self.epsilon)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        max_epochs = self.trainer.max_epochs\n",
    "        current_epoch = self.current_epoch\n",
    "        if self.eps_scheduler is None or self.eps_scheduler == \"static\":\n",
    "            self.epsilon = self.glob_epsilon\n",
    "        elif self.eps_scheduler == \"linear\":\n",
    "            self.epsilon = self.glob_epsilon * ((current_epoch + 1) / max_epochs)\n",
    "        elif self.eps_scheduler == \"log\":\n",
    "            self.epsilon = self.glob_epsilon * (np.log(current_epoch + 1) / np.log(max_epochs))\n",
    "        else:\n",
    "            raise ValueError(f\"config['eps_scheduler'] should be one of `[None, 'static', 'linear', 'log']`, but got `{self.eps_scheduler}`\")\n",
    "\n",
    "        self.log('train/eps', self.epsilon)\n",
    "        # self.model_dup.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        x, y = batch\n",
    "        # define optimizers\n",
    "        opt_1, opt_2 = self.optimizers()\n",
    "\n",
    "        ##########################\n",
    "        # predictor optimization #\n",
    "        ##########################\n",
    "        use_grad(self, requires_grad=True)\n",
    "        l_1 = self.predictor_loss(x, y, 'org')\n",
    "        p_loss = self.predictor_step(l_1, opt=opt_1)\n",
    "        self.log('train/p_loss', p_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        # update predictive weights\n",
    "        self.model_dup.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        ########################\n",
    "        # adversarial training #\n",
    "        ########################\n",
    "        use_grad(self, requires_grad=False)\n",
    "        self.bilevel_adv_training(batch, opt_1)\n",
    "\n",
    "        ##########################\n",
    "        # explainer optimization #\n",
    "        ##########################\n",
    "        use_grad(self.model_dup.predictor, self.model_dup.pred_linear, requires_grad=False)\n",
    "        use_grad(self.model.explainer, requires_grad=True)\n",
    "\n",
    "        l_2, l_3 = self.explainer_loss(x)\n",
    "        e_loss = self.explainer_step(l_2, l_3, opt=opt_2)\n",
    "        self.log('train/e_loss', e_loss, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "        # update explainer weight\n",
    "        # self.model.explainer.load_state_dict(self.model_dup.explainer.state_dict())\n",
    "\n",
    "        # Logging to TensorBoard by default\n",
    "        self._logging_loss(l_1, l_2, l_3, stage='train', on_step=False)\n",
    "\n",
    "    def _adv_validity(self, cf, y_prime):\n",
    "        def robust_validity(model, cf, y_prime):\n",
    "            cf_y_2 = model.predict(cf)\n",
    "            return accuracy(cf_y_2, y_prime.int())\n",
    "\n",
    "        w_1_val_arr, w_all_val_arr = [], []\n",
    "        for model in self.adv_val_models['w=1']:\n",
    "            w_1_validity = robust_validity(model, cf, y_prime)\n",
    "            w_1_val_arr.append(w_1_validity)\n",
    "        for model in self.adv_val_models['w=all']:\n",
    "            w_all_validity = robust_validity(model, cf, y_prime)\n",
    "            w_all_val_arr.append(w_all_validity)\n",
    "        return {\n",
    "            'val/w=1_validity': np.average(w_1_val_arr),\n",
    "            'val/w=all_validity': np.average(w_all_val_arr),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export net\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(input_dim, out_dim),\n",
    "            # nn.BatchNorm1d(num_features=out_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class MultilayerPerception(nn.Module):\n",
    "    def __init__(self, dims=[3, 100, 10]):\n",
    "        super().__init__()\n",
    "        layers  = []\n",
    "        num_blocks = len(dims)\n",
    "        for i in range(1, num_blocks):\n",
    "            layers += [\n",
    "                LinearBlock(dims[i-1], dims[i])\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class BaselineModel(PredictiveTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0]\n",
    "        self.model = nn.Sequential(\n",
    "            MultilayerPerception(self.enc_dims),\n",
    "            MultilayerPerception(self.dec_dims),\n",
    "            nn.Linear(self.dec_dims[-1], 1)\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        # x = ([],)\n",
    "        x, = x\n",
    "        y_hat = torch.sigmoid(self.model(x))\n",
    "        return torch.squeeze(y_hat, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export net\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, out_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_features=out_dim),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class MultilayerConv(nn.Module):\n",
    "    def __init__(self, dims=[3, 100, 10]):\n",
    "        super().__init__()\n",
    "        layers  = []\n",
    "        num_blocks = len(dims)\n",
    "        for i in range(1, num_blocks):\n",
    "            layers += [\n",
    "                ConvBlock(dims[i-1], dims[i])\n",
    "            ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export net\n",
    "\n",
    "class _CounterfactualModel(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        enc_dims = config['enc_dims'] if 'enc_dims' in config.keys() else []\n",
    "        dec_dims = config['dec_dims'] if 'dec_dims' in config.keys() else []\n",
    "        exp_dims = config['exp_dims'] if 'exp_dims' in config.keys() else []\n",
    "\n",
    "        assert enc_dims[-1] == dec_dims[0], f\"(enc_dims[-1]={enc_dims[-1]}) != (exp_dims[0]={dec_dims[0]})\"\n",
    "        assert enc_dims[-1] == exp_dims[0], f\"(enc_dims[-1]={enc_dims[-1]}) != (exp_dims[0]={exp_dims[0]})\"\n",
    "\n",
    "        self.encoder_model = MultilayerPerception(enc_dims)\n",
    "        # predictor\n",
    "        self.predictor = MultilayerPerception(dec_dims)\n",
    "        self.pred_linear = nn.Linear(dec_dims[-1], 1)\n",
    "        # explainer\n",
    "        exp_dims = [x for x in exp_dims]\n",
    "        exp_dims[0] = exp_dims[0] + dec_dims[-1]\n",
    "\n",
    "        self.explainer = nn.Sequential(\n",
    "            MultilayerPerception(exp_dims),\n",
    "            nn.Linear(exp_dims[-1], enc_dims[0])\n",
    "        )\n",
    "\n",
    "    def latent_vec(self, x):\n",
    "        z = self.encoder_model(x)\n",
    "        pred = self.predictor(z)\n",
    "        return z, pred\n",
    "\n",
    "    def latent_output(self, z, pred):\n",
    "        # prediction\n",
    "        y_hat = self.latent_y_hat_output(pred)\n",
    "        # counterfactual example\n",
    "        z_p = torch.cat((z, pred), -1)\n",
    "        cf = self.explainer(z_p)\n",
    "        return y_hat, cf\n",
    "\n",
    "    def latent_y_hat_output(self, pred):\n",
    "        y_hat = torch.sigmoid(self.pred_linear(pred))\n",
    "        return torch.squeeze(y_hat, -1)\n",
    "\n",
    "    def latent_cf_output(self, z_p):\n",
    "        return self.explainer(z_p)\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        z, pred = self.latent_vec(x)\n",
    "        return self.latent_output(z, pred)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # don't use this method unless the `forward` function is needed\n",
    "        return self.model_forward(x)\n",
    "\n",
    "\n",
    "class _CounterfactualFramework(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        enc_dims = config['enc_dims'] if 'enc_dims' in config.keys() else []\n",
    "        dec_dims = config['dec_dims'] if 'dec_dims' in config.keys() else []\n",
    "        exp_dims = config['exp_dims'] if 'exp_dims' in config.keys() else []\n",
    "\n",
    "        assert enc_dims[-1] == dec_dims[0], f\"(enc_dims[-1]={enc_dims[-1]}) != (exp_dims[0]={dec_dims[0]})\"\n",
    "        assert enc_dims[-1] == exp_dims[0], f\"(enc_dims[-1]={enc_dims[-1]}) != (exp_dims[0]={exp_dims[0]})\"\n",
    "\n",
    "        # predictor\n",
    "        self.predictor = MultilayerPerception(enc_dims + dec_dims[1:])\n",
    "        self.pred_linear = nn.Linear(dec_dims[-1], 1)\n",
    "        # explainer\n",
    "        # exp_dims = [x for x in exp_dims]\n",
    "        # exp_dims[0] = exp_dims[0] + dec_dims[-1]\n",
    "\n",
    "        self.explainer = nn.Sequential(\n",
    "            MultilayerPerception(enc_dims + exp_dims[1:]),\n",
    "            nn.Linear(exp_dims[-1], enc_dims[0])\n",
    "        )\n",
    "\n",
    "    def latent_vec(self, x):\n",
    "        pred = self.predictor(x)\n",
    "        return x, pred\n",
    "\n",
    "    def latent_output(self, x, pred):\n",
    "        # prediction\n",
    "        y_hat = self.latent_y_hat_output(pred)\n",
    "        # counterfactual example\n",
    "        cf = self.explainer(x)\n",
    "        return y_hat, cf\n",
    "\n",
    "    def latent_y_hat_output(self, pred):\n",
    "        y_hat = torch.sigmoid(self.pred_linear(pred))\n",
    "        return torch.squeeze(y_hat, -1)\n",
    "\n",
    "    def latent_cf_output(self, z_p):\n",
    "        return self.explainer(z_p)\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        z, pred = self.latent_vec(x)\n",
    "        return self.latent_output(z, pred)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # don't use this method unless the `forward` function is needed\n",
    "        return self.model_forward(x)\n",
    "\n",
    "\n",
    "\n",
    "class CounterfactualModel(CounterfactualTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = _CounterfactualModel(config)\n",
    "\n",
    "    def latent_vec(self, x):\n",
    "        return self.model.latent_vec(x)\n",
    "\n",
    "    def latent_output(self, z, pred):\n",
    "        return self.model.latent_output(z, pred)\n",
    "\n",
    "    def latent_y_hat_output(self, pred):\n",
    "        return self.model.latent_y_hat_output(pred)\n",
    "\n",
    "    def latent_cf_output(self, z_p):\n",
    "        return self.model.latent_cf_output(z_p)\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        return self.model.model_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export net\n",
    "\n",
    "class AdvCounterfactualModel(AdvBiLevelCounterfactualTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = _CounterfactualModel(config)\n",
    "        self.model_dup = _CounterfactualModel(config)\n",
    "        self.model_dup.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def latent_vec(self, x):\n",
    "        return self.model.latent_vec(x)\n",
    "\n",
    "    def latent_output(self, z, pred):\n",
    "        return self.model.latent_output(z, pred)\n",
    "\n",
    "    def latent_y_hat_output(self, pred):\n",
    "        return self.model.latent_y_hat_output(pred)\n",
    "\n",
    "    def latent_cf_output(self, z_p):\n",
    "        return self.model.latent_cf_output(z_p)\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        return self.model.model_forward(x)\n",
    "\n",
    "class AdvCounterfactualFramework(AdvBiLevelCounterfactualTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = _CounterfactualFramework(config)\n",
    "        self.model_dup = _CounterfactualFramework(config)\n",
    "        self.model_dup.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def latent_vec(self, x):\n",
    "        return self.model.latent_vec(x)\n",
    "\n",
    "    def latent_output(self, z, pred):\n",
    "        return self.model.latent_output(z, pred)\n",
    "\n",
    "    def latent_y_hat_output(self, pred):\n",
    "        return self.model.latent_y_hat_output(pred)\n",
    "\n",
    "    def latent_cf_output(self, z_p):\n",
    "        return self.model.latent_cf_output(z_p)\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        return self.model.model_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export net\n",
    "class ConvCounterNet(CounterfactualTrainingModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        assert self.enc_dims[-1] == self.dec_dims[0], f\"(enc_dims[-1]={self.enc_dims[-1]}) != (exp_dims[0]={self.dec_dims[0]})\"\n",
    "        assert self.enc_dims[-1] == self.exp_dims[0], f\"(enc_dims[-1]={self.enc_dims[-1]}) != (exp_dims[0]={self.enc_dims[0]})\"\n",
    "\n",
    "        self.encoder_model = MultilayerConv(self.enc_dims)\n",
    "        # predictor\n",
    "        self.predictor = MultilayerConv(self.dec_dims)\n",
    "        self.pred_linear = nn.Linear(self.dec_dims[-1], 1)\n",
    "        # explainer\n",
    "        exp_dims = [x for x in self.exp_dims]\n",
    "        exp_dims[0] = self.exp_dims[0] + self.dec_dims[-1]\n",
    "\n",
    "        self.explainer = nn.Sequential(\n",
    "            MultilayerPerception(exp_dims),\n",
    "            nn.Linear(self.exp_dims[-1], self.enc_dims[0])\n",
    "        )\n",
    "\n",
    "    def model_forward(self, x):\n",
    "        x = x.unsqueeze(dim=-1)\n",
    "        x = self.encoder_model(x)\n",
    "        # predicted y_hat\n",
    "        pred = self.predictor(x)\n",
    "        y_hat = torch.sigmoid(self.pred_linear(pred.squeeze(-1)))\n",
    "        # counterfactual example\n",
    "        x = torch.cat((x, pred), 1).squeeze(-1)\n",
    "        c = self.explainer(x)\n",
    "        return torch.squeeze(y_hat, -1), c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
