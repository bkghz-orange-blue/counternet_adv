# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_adversarial.ipynb (unless otherwise specified).

__all__ = ['adv_validity', 'avg_decrease', 'avg_validity_changes', 'ExperimentLogger', 'ExperimentLoggerLocal',
           'ExperimentLoggerWanb', 'experiment_step', 'local_explainer_experiment_step']

# Cell
from optparse import Option
from .training_module import BaseModule
from .import_essentials import *
from .train import train_model
from .net import AdvCounterfactualModel, CounterfactualModel
from .evaluate import cf_gen_parallel, model_cf_gen, load_trained_model
from .interface import LocalExplainerBase
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

from torchmetrics.functional.classification import accuracy

# Cell
def adv_validity(m1: CounterfactualModel, m2: CounterfactualModel, x_1: torch.Tensor, cf_1: torch.Tensor):
    """check validity of the updated model

    Args:
        m1 (BaseModule): original model
        m2 (BaseModule): updated model

    Returns:
        [float]: return validity score of m1's CF on m2
    """
    y_prime = torch.ones(len(x_1)) - m1.predict(x_1)
    # cf_y_1 = m1.predict(cf)
    cf_y_2 = m2.predict(cf_1)

    # val_1 = accuracy(y_prime, cf_y_1.int())
    val_2 = accuracy(y_prime, cf_y_2.int())

    # print(f"val_1: {val_1}, val_2: {val_2}")
    return val_2.item()

# Cell
def avg_decrease(matrix: np.ndarray):
    if not isinstance(matrix, (list, np.ndarray)):
        raise ValueError(f"`matrix` needs to be a list or numpy array, but got {type(matrix)}.")
    result = {}
    n_datasets = len(matrix[0])

    validity = [matrix[i][i] for i in range(n_datasets)]
    result.update({
        'cf_validity': {
            'avg': np.average(validity), 'std': np.std(validity)
        }
    })

    w1_result = []
    for i in range(n_datasets - 1):
        w1_result += [matrix[i][i] - matrix[i][i+1]]
    result.update({
        'w=1': {
            'avg': np.average(w1_result), 'std': np.std(w1_result)
        }
    })

    wall_result =  []
    for i in range(n_datasets):
        for j in range(n_datasets):
            if j == i:
                continue
            wall_result += [matrix[i][i] - matrix[i][j]]
    result.update({
        'all': {
            'avg': np.average(wall_result), 'std': np.std(wall_result)
        }
    })

    for k in result.keys():
        print(f"[{k}] avg={result[k]['avg']:.4f}; std={result[k]['std']:.4f}")
    return result

# Cell
def avg_validity_changes(matrix: np.ndarray):
    if not isinstance(matrix, (list, np.ndarray)):
        raise ValueError(f"`matrix` needs to be a list or numpy array, but got {type(matrix)}.")
    result = {}
    n_datasets = len(matrix[0])

    validity = [matrix[i][i] for i in range(n_datasets)]

    w1_val_result, w1_dec_result = [], []
    for i in range(n_datasets - 1):
        w1_val_result.append(matrix[i][i+1])
        w1_dec_result.append(matrix[i][i] - matrix[i][i+1])

    wall_val_result, wall_dec_result =  [], []
    for i in range(n_datasets):
        for j in range(n_datasets):
            if j == i: continue
            wall_val_result.append(matrix[i][j])
            wall_dec_result.append(matrix[i][i] - matrix[i][j])
    result = {
        'cf_validity': { 'mean': np.average(validity), 'std': np.std(validity) },
        'cf_validity (w=1)': { 'mean': np.average(w1_val_result), 'std': np.std(w1_val_result) },
        'cf_validity (all)': { 'mean': np.average(wall_val_result), 'std': np.std(wall_val_result) },
        'validity_decrease (w=1)': { 'mean': np.average(w1_dec_result), 'std': np.std(w1_dec_result) },
        'validity_decrease (all)': { 'mean': np.average(wall_dec_result), 'std': np.std(wall_dec_result) }
    }

    return pd.DataFrame.from_dict(result)

# Cell
def _useful_result(result: Dict[str, Any]) -> Dict[str, Any]:
    # average_time,pred_accuracy,validity,proximity
    useful_metrics = ["average_time", "pred_accuracy", "validity", "proximity"]
    useful_results = {metric: result[metric] for metric in useful_metrics}
    # result.pop('x', None); result.pop('cf', None)
    # result.pop('y_prime', None); result.pop('cf_y', None)
    return useful_results


def _logger_name(data_dir: str) -> str:
    return data_dir.split("assets/data/")[1].split(".csv")[0]

# Cell
def _aggregate_default_data_encoders(default_model_config: Dict[str, Any], data_dir_list: List[str]):
    # data encoding
    data = pd.concat(
        [pd.read_csv(data_dir) for data_dir in data_dir_list]
    )
    pl_logger.info(f"total data length: {len(data)}")
    if len(default_model_config['continous_cols']) != 0:
        pl_logger.info("preprocessing continuous features...")
        normalizer = MinMaxScaler().fit(
            data[default_model_config['continous_cols']]
        )
        default_model_config.update({"normalizer": normalizer})

    if len(default_model_config['discret_cols']) != 0:
        pl_logger.info("preprocessing discret features...")
        encoder = OneHotEncoder(sparse=False).fit(
            data[default_model_config['discret_cols']]
        )
        default_model_config.update({"encoder": encoder})
    return default_model_config


def _set_val_model_dict(val_models: List[Any], index: int, length: int):
    w_1_models = list()
    if index > 0:
        w_1_models.append(val_models[index - 1])
    if index < length - 1:
        w_1_models.append(val_models[index + 1])

    w_all_models = deepcopy(val_models)
    w_all_models.pop(index)
    return { "w=1": w_1_models, "w=all": w_all_models}


def _train_models(
    default_model_config: Dict[str, Any],
    t_config: Dict[str, Any],
    data_dir_list: List[str],
    module: CounterfactualModel,
    use_prev_model_weights: bool = False,
    return_best_model: bool = False, # return last model by default
    tb_logger_dir: Optional[str] = "",
    save_models_to_assets: bool = False
) -> List[str]:
    model_list = []
    model_checkpoints_list = []
    last_model_path = None

    for i, data_dir in enumerate(data_dir_list):
        model_config = deepcopy(default_model_config)
        model_config['data_dir'] = data_dir
        if 'adv_val_models' not in model_config.keys():
            model_config['adv_val_models'] = None
        if model_config['adv_val_models']: # not None
            model_config['adv_val_models'] = _set_val_model_dict(model_config['adv_val_models'], i, len(data_dir_list))

        if not use_prev_model_weights:
            # train model from scratch
            training_results = train_model(
                module(model_config),
                t_config, logger_name=f"{tb_logger_dir}{_logger_name(data_dir)}",
                return_best_model_path=True
            )
        else:
            # use last trained model as the init weight
            last_model = module(model_config).load_from_checkpoint(last_model_path, **model_config)
            training_results = train_model(
                last_model,
                t_config, logger_name=f"{_logger_name(data_dir)}",
                return_best_model_path=True
            )
        last_model_path = training_results['best_model_path']
        _model = training_results['model']
        if return_best_model:
            _model.load_from_checkpoint(last_model_path)
            if save_models_to_assets:
                shutil.copy(last_model_path, f'assets/weights/')

        model_list.append(_model)
        # best_model = module(model_config).load_from_checkpoint(last_model_path, **model_config)
        model_checkpoints_list.append(last_model_path)

    assert len(model_checkpoints_list) == len(data_dir_list)
    # return model_checkpoints_list
    return model_list


def _calculate_validity_matrix(model_list: List[BaseModule], cf_results: Dict[str, Any]):
    validity_matrix = []
    n_models = len(model_list)
    for i in range(n_models):
        result_i = []
        for j in range(n_models):
            _cf_result = cf_results[f"m_{i}"]
            val = adv_validity(model_list[i], model_list[j], x_1=_cf_result['x'], cf_1=_cf_result['cf'])
            result_i.append(val)
        validity_matrix.append(result_i)
    return validity_matrix


def _store_results(results: Dict[str, Any]) -> None:
    data_name = results['data_name']
    cf_results = results['cf_results']
    validity_matrix = results['validity_matrix']
    model_list = results['model_list']
    hparam = results['hparams']
    # create default directory
    if not Path(f'assets/result/{data_name}').exists():
        Path(f'assets/result/{data_name}').mkdir()

    # create logging directory
    experiment_versions = []
    adv = results['adv']
    for p in Path(f'assets/result/{data_name}').iterdir():
        _adv, _, n_version = p.name.split('_')
        if _adv == 'adv=True' and adv:
            experiment_versions.append(int(n_version))
        if (_adv == 'adv=False') and (not adv):
            experiment_versions.append(int(n_version))

    sorted_experiment_versions = sorted(experiment_versions)
    if len(sorted_experiment_versions) == 0:
        sorted_experiment_versions = [-1]
    n_version = sorted_experiment_versions[-1] + 1

    dir_path = Path(f'assets/result/{data_name}/adv={adv}_version_{n_version}')
    dir_path.mkdir(exist_ok=False)

    # store hparam
    with open(dir_path / 'hparam.yml', 'w') as f:
        yaml.dump(hparam, f, default_flow_style=False)

    # store validity matrix
    col_names = [f'm_{i}' for i in range(len(model_list))]
    pd.DataFrame(validity_matrix, columns=col_names, index=col_names).to_csv(dir_path / 'validity_matrix.csv')

    # store cf results (e.g., proximity)
    pd.DataFrame.from_dict(cf_results, orient='index').to_csv(dir_path / 'metrics.csv')
    return dir_path

# Cell
class ExperimentLogger(ABC):
    def store_results(self, results: Dict[str, Any]) -> None:
        raise NotImplementedError

class ExperimentLoggerLocal(ExperimentLogger):
    def __init__(self) -> None:
        pass

    def _create_logging_dir(self, dataset_path: Path, adv_training: bool) -> Path:
        # find experiment version
        experiment_versions = []
        for p in dataset_path.iterdir():
            _adv, _, n_version = p.name.split('_')
            if _adv == 'adv=True' and adv_training:
                experiment_versions.append(int(n_version))
            if (_adv == 'adv=False') and (not adv_training):
                experiment_versions.append(int(n_version))

        sorted_experiment_versions = sorted(experiment_versions)
        if len(sorted_experiment_versions) == 0:
            sorted_experiment_versions = [-1]
        n_version = sorted_experiment_versions[-1] + 1

        # create new dir
        dir_path = dataset_path / Path(f"adv={adv_training}_version_{n_version}")
        dir_path.mkdir(exist_ok=False)
        return dir_path

    def store_results(self, results: Dict[str, Any]) -> None:
        data_name = results['data_name']
        cf_results = results['cf_results']
        validity_matrix = results['validity_matrix']
        model_list = results['model_list']
        hparam = results['hparams']
        dataset_path = Path(f'assets/result/{data_name}')

        # create default directory
        if not dataset_path.exists():
            dataset_path.mkdir()

        # create logging directory
        adv = results['adv']
        dir_path = self._create_logging_dir(dataset_path=dataset_path, adv_training=adv)

        # store hparam
        with open(dir_path / 'hparam.yml', 'w') as f:
            yaml.dump(hparam, f, default_flow_style=False)

        # store validity matrix
        col_names = [f'm_{i}' for i in range(len(model_list))]
        pd.DataFrame(validity_matrix, columns=col_names, index=col_names).to_csv(dir_path / 'validity_matrix.csv')

        # store cf results (e.g., proximity)
        cf_results = {m: _useful_result(cf_results[m]) for m in col_names }
        pd.DataFrame.from_dict(cf_results, orient='index').to_csv(dir_path / 'metrics.csv')
        return dir_path

# Cell
class ExperimentLoggerWanb(ExperimentLogger):
    def __init__(self, logger_name: str) -> None:
        super().__init__()
        # self.logger_name = logger_name
        self.run = wandb.init(project="adv-counternet", entity="birkhoffg", name=logger_name, settings=wandb.Settings(start_method="fork"))

    def store_results(self, results: Dict[str, Any]) -> None:
        data_name = results['data_name']
        cf_results = results['cf_results']
        validity_matrix = results['validity_matrix']
        model_list = results['model_list']
        hparam = results['hparams']
        t_config = results['trainer_configs']

        # with wandb.init(project="my-test-project", entity="birkhoffg", name=self.logger_name, settings=wandb.Settings(start_method="fork")) as run:
        with self.run as run:
            # store hparam
            run.config.update(hparam)
            run.config.update(t_config)

            # store cf results (e.g., proximity)
            col_names = [f'm_{i}' for i in range(len(model_list))]
            cf_results = {m: _useful_result(cf_results[m]) for m in col_names }
            cf_results_df = pd.DataFrame.from_dict(cf_results, orient='index')
            # cf metric stats
            cf_aggre_df = cf_results_df.describe().loc[['mean', 'std']].reset_index().rename(columns={'index': 'stat'})
            cf_results_df = cf_results_df.reset_index().rename(columns={'index': 'model_name'})
            cf_results_table = wandb.Table(dataframe=cf_results_df)
            cf_aggre_table = wandb.Table(dataframe=cf_aggre_df)

            # store validity matrix
            val_matrix_df = pd.DataFrame(validity_matrix, columns=col_names, index=col_names)
            val_matrix_df = val_matrix_df.reset_index().rename(columns={'index': 'model_name'})
            val_matrix_table = wandb.Table(dataframe=val_matrix_df)
            val_matrix_heatmap = wandb.plots.HeatMap(x_labels=col_names, y_labels=col_names, matrix_values=validity_matrix, show_text=False)

            # avg validity changes
            avg_val_changes_df = avg_validity_changes(validity_matrix).reset_index().rename(columns={'index': 'stat'})
            avg_val_changes_table = wandb.Table(dataframe=avg_val_changes_df)

            run.log({
                'Validity Matrix': val_matrix_table, 'Heatmap': val_matrix_heatmap,
                'CF Results': cf_results_table, 'Validity Changes': avg_val_changes_table,
                'CF Metrics': cf_aggre_table
            })
        return self.run.dir

        # self.run.finish()

# Cell
def experiment_step(
    default_model_config: Dict[str, Any],
    t_config: Dict[str, Any],
    data_dir_list: List[str],
    module: CounterfactualModel,
    use_prev_model_weights: bool = False,
    return_best_model: bool = False, # return last model by default
    tb_logger_dir: Optional[str] = "",
    save_models_to_assets: bool = False,
    experiment_logger: Optional[ExperimentLogger] = None
) -> Dict[str, Any]:
    """set up experiment for CounterNet on distributional shifted datasets

    Args:
        default_model_config (Dict[str, Any]): model configuration for CounterNet
        t_config (Dict[str, Any]): training configuration
        data_dir_list (List[str]): a list of dataset paths (e.g., `["w_1.csv", "w_2.csv"]`);
            the length of `data_dir_list` should be equal to the length of returned `model_list`
        module (CounterfactualModel): CounterfactualModel to be trained
        use_prev_model_weights (bool, optional): define whether to use the previously trained model
            as a starting point (True) or not (False). Defaults to False.
        return_best_model (bool, optional): return the best model (according to the validation set, True)
            or the model trained in the last epoch (False). Defaults to False.

    Returns:
        Dict[str, Any]: the result of the experiment
            * `hparams`: hyperparameters for training the model
            * `cf_results`: a list of cf results (e.g., validity, proximity) on the original datasets for each model
            * `validity_matrix`: the validity matrix of each CounterNet models on all other models
            * `model_list`: the list of CounterNet model
    """
    hparams = deepcopy(default_model_config)
    hparams.update(t_config)
    # data encoding
    pl_logger.info("aggregating data...")
    default_model_config = _aggregate_default_data_encoders(default_model_config, data_dir_list)

    # trained model
    pl_logger.info("start training...")
    model_list = _train_models(default_model_config, t_config,
        data_dir_list, module, use_prev_model_weights, return_best_model, tb_logger_dir)

    # extract models from `model_checkpoints_list`
    pl_logger.info("extracting useful cf results...")
    # _model_config = deepcopy(default_model_config)
    # model_list = [
    #     module(_model_config).load_from_checkpoint(checkpoint_path, **_model_config)
    #         for checkpoint_path in model_checkpoints_list
    # ]

    cf_results = {}
    for i, m in enumerate(model_list):
        cf_results[f'm_{i}'] = model_cf_gen(m) #_useful_result(model_cf_gen(m))

    # calculate the validity matrix
    pl_logger.info("calculating validity matrix...")
    validity_matrix = _calculate_validity_matrix(model_list, cf_results)

    # store result
    results = {
        'adv': default_model_config['adv'],
        'data_name': default_model_config['data_name'],
        'hparams': hparams,
        'trainer_configs': t_config,
        'cf_results': cf_results,
        'validity_matrix': validity_matrix,
        'model_list': model_list,
        # 'result_path': path
    }
    if experiment_logger is not None:
        dir_path = experiment_logger.store_results(results)
        print(f"Results stored at {dir_path}.")
        # _store_results(results)

    return results

# Cell
def local_explainer_experiment_step(default_model_config: Dict[str, Any],
                                    t_config: Dict[str, Any],
                                    data_dir_list: List[str],
                                    pred_module: BaseModule,
                                    cf_params: Dict[str, Any],
                                    cf_module: LocalExplainerBase,
                                    is_parallel: bool = True,
                                    test_size: Optional[int] = None,
                                    use_prev_model_weights: bool = False,
                                    return_best_model: bool = False, # return last model by default
                                    experiment_logger: Optional[ExperimentLogger] = None) -> Dict[str, Any]:

    hparams = deepcopy(default_model_config)
    # data encoding
    pl_logger.info("aggregating data...")
    default_model_config = _aggregate_default_data_encoders(default_model_config, data_dir_list)

    # trained model
    pl_logger.info("start training...")
    model_list = _train_models(default_model_config, t_config,
        data_dir_list, pred_module, use_prev_model_weights, return_best_model)

    # extract models from `model_checkpoints_list`
    pl_logger.info("extracting useful cf results...")

    cf_results = {}
    for i, m in enumerate(model_list):
        print(data_dir_list[i])
        cf_params.update({'model': m})
        _cf_result = cf_gen_parallel(cf_params, cf_module,is_parallel, test_size)
        # _cf_result = model_cf_gen(m)
        cf_results[f'm_{i}'] = _cf_result

    # calculate the validity matrix
    pl_logger.info("calculating validity matrix...")
    # validity_matrix = _calculate_validity_matrix(model_list)
    validity_matrix = _calculate_validity_matrix(model_list, cf_results)

    # store result
    results = {
        'adv': default_model_config['adv'],
        'data_name': default_model_config['data_name'],
        'hparams': hparams,
        'trainer_configs': {},
        'cf_results': cf_results,
        'validity_matrix': validity_matrix,
        'model_list': model_list,
        # 'result_path': path
    }
    if experiment_logger:
        dir_path = experiment_logger.store_results(results)
        print(f"Results stored at {dir_path}.")

    return results