# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_train.ipynb (unless otherwise specified).

__all__ = ['train_model']

# Cell

from .import_essentials import *
from .training_module import BaseModule

# Cell
def _has_no_model_checkpoint(callbacks: List[Callback]) -> bool:
    for callback in callbacks:
        if isinstance(callback, ModelCheckpoint):
            return False
    return True


def train_model(model: BaseModule,
          t_configs: Dict[str, Any],
          callbacks: Optional[List[Callback]] = None,
          resume_from_checkpoints: Optional[Any] = None,
          description: Optional[str] = None,
          debug: Optional[bool] = False,
          logger: Optional[Union[LightningLoggerBase, bool]] = None,
          logger_name: str = "debug",
          return_trainer: bool = False,
          return_best_model_path: bool = False,
          best_model_dir: Optional[Path] = None):
    # logger
    if logger is None:
        logger = pl_loggers.TestTubeLogger(
            Path('log/'), name=logger_name,
            description=description, debug=debug, log_graph=True
        )

    # modelcheckpoint
    checkpoint_callback = ModelCheckpoint(
        monitor='val/val_loss', save_top_k=3, mode='min'
    )

    def save_best_model(dir_path: Path):
        if not Path(dir_path).is_dir():
            pl_logger.warning(f"'{dir_path}' is not a directory, creating a directory...")
        dir_path.mkdir(parents=True, exist_ok=True)
        best_model_path = Path(checkpoint_callback.best_model_path)
        shutil.copy(best_model_path, dir_path)
        return best_model_path

    # define callbacks
    if callbacks is None:
        callbacks = [checkpoint_callback]
    elif _has_no_model_checkpoint(callbacks):
        callbacks += [checkpoint_callback]

    # train the model
    trainer = pl.Trainer(logger=logger, callbacks=callbacks, **t_configs)

    pl_logger.info(f'hyper parameters: {model.hparams}')

    trainer.fit(model)
    if best_model_dir:
        save_best_model(best_model_dir)
        pl_logger.info(f'best model is saved at {best_model_dir}.')

    if return_trainer:
        return {'model': model, 'trainer': trainer}
    elif return_best_model_path:
        return {'model': model, 'best_model_path': checkpoint_callback.best_model_path}
    else:
        return model